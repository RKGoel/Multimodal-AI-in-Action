{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b189300-0a5e-4fdf-8662-b87238a29e45",
   "metadata": {},
   "source": [
    "# Chapter 3: Generating Images from Text Using DALL-E 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866743e-e460-47ca-b3b0-cdf171d4d4f6",
   "metadata": {},
   "source": [
    "## Downloading and Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a655c8-27cd-4f5f-ae3b-1c6264c644ab",
   "metadata": {},
   "source": [
    "To download the dataset, visit the link https://cocodataset.org/#download and download the training and validation images for 2014 version along with their annotation files. The annotation files contain the mapping of image IDs to their captions. We will be training our model on the training data while monitoring the validation loss and once the model is trained, we can use it for zero-shot inferences from any data, hence we would not need test images. The MS-COCO dataset released in 2014 contains 164K images split into training (83K), validation (41K) and test (41K) sets. When you would download the dataset, it would download zip files for all the images, which are of different sizes, as well as the annotation files. Unzip the dataset into data/ directory. To load the (image, caption) pairs, we will create PyTorch's data loaders using datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a7e137-db98-4039-b6ba-58e6bdda717e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.3.1\n",
      "  Using cached torch-2.3.1-cp310-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (2025.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from sympy->torch==2.3.1) (1.3.0)\n",
      "Using cached torch-2.3.1-cp310-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.17.2 requires torch==2.2.2, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.3.1\n",
      "Requirement already satisfied: torchvision==0.17.2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision==0.17.2) (2.2.3)\n",
      "Collecting torch==2.2.2 (from torchvision==0.17.2)\n",
      "  Using cached torch-2.2.2-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision==0.17.2) (11.1.0)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (2025.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from jinja2->torch==2.2.2->torchvision==0.17.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from sympy->torch==2.2.2->torchvision==0.17.2) (1.3.0)\n",
      "Using cached torch-2.2.2-cp310-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "Successfully installed torch-2.2.2\n",
      "Requirement already satisfied: pycocotools in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from pycocotools) (3.10.0)\n",
      "Requirement already satisfied: numpy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from pycocotools) (2.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n",
      "Requirement already satisfied: transformers in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: timm in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (1.0.14)\n",
      "Requirement already satisfied: torch in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (0.17.2)\n",
      "Requirement already satisfied: pyyaml in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (0.29.0)\n",
      "Requirement already satisfied: safetensors in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (0.5.2)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->timm) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->timm) (3.1.5)\n",
      "Requirement already satisfied: numpy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision->timm) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision->timm) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Collecting numpy<2.0\n",
      "  Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -U torch==2.3.1\n",
    "! pip3 install torchvision==0.17.2\n",
    "! pip3 install pycocotools\n",
    "! pip3 install transformers\n",
    "! pip install timm\n",
    "! pip install \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98eecacc-4d35-4ca8-b051-6a0525d99653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9dc95e5-9c6b-48e6-b8a4-a8275d9983a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transforms\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3667bc5b-add5-4338-81b6-0940bcf9c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "train_dir = '../../Personal Git Repo/data/train2014'\n",
    "val_dir = '../../Personal Git Repo/data/val2014'\n",
    "train_ann_file = '../../Personal Git Repo/data/annotations/captions_train2014.json'\n",
    "val_ann_file = '../../Personal Git Repo/data/annotations/captions_val2014.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73552e7-0e47-4212-967a-6557992b486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Images are <class 'PIL.Image.Image'> before transformation\n",
    "train_data = datasets.CocoCaptions(root=train_dir, annFile=train_ann_file, transform=image_transform)\n",
    "val_data = datasets.CocoCaptions(root=val_dir, annFile=val_ann_file, transform=image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13031c9c-0f93-46fc-966b-ffc9df6aec0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  82783\n",
      "Image Size:  torch.Size([3, 224, 224])\n",
      "Image  <class 'torch.Tensor'>\n",
      "['A zebra grazing on lush green grass in a field.', 'Zebra reaching its head down to ground where grass is. ', 'The zebra is eating grass in the sun.', 'A lone zebra grazing in some green grass.', 'a Zebra grazing on grass in a green open field.']\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: ', len(train_data))\n",
    "img, target = train_data[3] # load 4th sample\n",
    "\n",
    "print(\"Image Size: \", img.size())\n",
    "print(\"Image \", type(img))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba2956b-93e6-435c-8c3d-44e5f9a0b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCaptionsFlattened(torch.utils.data.Dataset):\n",
    "    def __init__(self, coco_captions):\n",
    "        self.coco_captions = coco_captions\n",
    "        print(\"Number of images:\", len(self.coco_captions))\n",
    "        caption_counts = [len(captions) for _, captions in coco_captions]\n",
    "        self.cumulative_counts = self._compute_cumulative_counts(caption_counts)\n",
    "        print(\"Number of image x caption pairs:\", self.cumulative_counts[-1])\n",
    "\n",
    "    def _compute_cumulative_counts(self, counts):\n",
    "        cumulative = [0]\n",
    "        for count in counts:\n",
    "            cumulative.append(cumulative[-1] + count)\n",
    "        return cumulative\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_counts[-1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Find the image index corresponding to the flattened caption index\n",
    "        image_idx = bisect.bisect_right(self.cumulative_counts, index) - 1\n",
    "        caption_idx = index - self.cumulative_counts[image_idx]\n",
    "        # print(index, image_idx, caption_idx)\n",
    "        image, captions = self.coco_captions[image_idx]\n",
    "        return image, captions[caption_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a02e4005-cc33-4276-8473-796c700283d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 82783\n",
      "Number of image x caption pairs: 414113\n"
     ]
    }
   ],
   "source": [
    "train_data_flattened = CocoCaptionsFlattened(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5602fa0-0046-4392-905a-4b1a9456a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_data_flattened, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963358e-2e1d-421d-b1e9-ad9f44e0c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_flattened = CocoCaptionsFlattened(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb8d1e44-e4ba-49fd-b0a7-744786f2e602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Size:  torch.Size([3, 224, 224])\n",
      "A flower vase is sitting on a porch stand.\n"
     ]
    }
   ],
   "source": [
    "img, caption = train_data_flattened[10] # load 11th sample\n",
    "\n",
    "print(\"Image Size: \", img.size())\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084656e-656e-492a-ab25-16441c0bc31a",
   "metadata": {},
   "source": [
    "## Load CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98c35639-4457-4c61-b460-cbbc7cff34a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (6.3.1)\n",
      "Requirement already satisfied: regex in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: wcwidth in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from ftfy) (0.2.13)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/84/h1vtggnn11g_xs1m3kzfns4h0000gn/T/pip-req-build-ynv3y0ni\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/84/h1vtggnn11g_xs1m3kzfns4h0000gn/T/pip-req-build-ynv3y0ni\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: regex in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from clip==1.0) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from clip==1.0) (0.17.2)\n",
      "Requirement already satisfied: wcwidth in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->clip==1.0) (2025.2.0)\n",
      "Requirement already satisfied: numpy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision->clip==1.0) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1833e25-2641-4f48-b1c1-f4c6d0b8b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "380ae998-e8af-4302-b0ee-2207769bb82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch import nn, einsum\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd7d37b0-fa03-4e02-873c-e59b8428679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_to(\n",
    "    image,\n",
    "    target_image_size,\n",
    "    clamp_range = None,\n",
    "    nearest = False,\n",
    "    **kwargs\n",
    "):\n",
    "    orig_image_size = image.shape[-1]\n",
    "\n",
    "    if orig_image_size == target_image_size:\n",
    "        return image\n",
    "\n",
    "    if not nearest:\n",
    "        scale_factors = target_image_size / orig_image_size\n",
    "        out = resize(image, scale_factors = scale_factors, **kwargs)\n",
    "    else:\n",
    "        out = F.interpolate(image, target_image_size, mode = 'nearest')\n",
    "\n",
    "    if exists(clamp_range):\n",
    "        out = out.clamp(*clamp_range)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1132e30-4dd5-4eaf-9e09-3da3b2ba8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClipAdapter(nn.Module):\n",
    "    def __init__(self, clip, **kwargs):\n",
    "        super().__init__()\n",
    "        self.clip = clip\n",
    "        self.overrides = kwargs\n",
    "\n",
    "    def validate_and_resize_image(self, image):\n",
    "        image_size = image.shape[-1]\n",
    "        assert image_size >= self.image_size, f'you are passing in an image of size {image_size} but CLIP requires the image size to be at least {self.image_size}'\n",
    "        return resize_image_to(image, self.image_size)\n",
    "\n",
    "    @property\n",
    "    def dim_latent(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def image_size(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def image_channels(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def max_text_len(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def embed_text(self, text):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def embed_image(self, image):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b886de9-1aab-4974-98e3-d98e0a813099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "EmbeddedText = namedtuple('EmbedTextReturn', ['text_embed', 'text_encodings'])\n",
    "EmbeddedImage = namedtuple('EmbedImageReturn', ['image_embed', 'image_encodings'])\n",
    "\n",
    "def l2norm(t):\n",
    "    return F.normalize(t, dim = -1)\n",
    "\n",
    "class OpenAIClipAdapter(BaseClipAdapter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name = 'ViT-B/32'\n",
    "    ):\n",
    "        import clip\n",
    "        openai_clip, preprocess = clip.load(name)\n",
    "        super().__init__(openai_clip)\n",
    "        self.eos_id = 49407 # for handling 0 being also '!'\n",
    "\n",
    "        text_attention_final = self.find_layer('ln_final')\n",
    "\n",
    "        self.dim_latent_ = text_attention_final.weight.shape[0]\n",
    "        self.handle = text_attention_final.register_forward_hook(self._hook)\n",
    "\n",
    "        self.clip_normalize = preprocess.transforms[-1]\n",
    "        self.cleared = False\n",
    "\n",
    "    def find_layer(self,  layer):\n",
    "        modules = dict([*self.clip.named_modules()])\n",
    "        return modules.get(layer, None)\n",
    "\n",
    "    def clear(self):\n",
    "        if self.cleared:\n",
    "            return\n",
    "\n",
    "        self.handle()\n",
    "\n",
    "    def _hook(self, _, inputs, outputs):\n",
    "        self.text_encodings = outputs\n",
    "\n",
    "    @property\n",
    "    def dim_latent(self):\n",
    "        return self.dim_latent_\n",
    "\n",
    "    @property\n",
    "    def image_size(self):\n",
    "        return self.clip.visual.input_resolution\n",
    "\n",
    "    @property\n",
    "    def image_channels(self):\n",
    "        return 3\n",
    "\n",
    "    @property\n",
    "    def max_text_len(self):\n",
    "        return self.clip.context_length\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_text(self, text):\n",
    "        text = text[..., :self.max_text_len]\n",
    "\n",
    "        is_eos_id = (text == self.eos_id)\n",
    "        text_mask_excluding_eos = is_eos_id.cumsum(dim = -1) == 0\n",
    "        text_mask = F.pad(text_mask_excluding_eos, (1, -1), value = True)\n",
    "        text_mask = text_mask & (text != 0)\n",
    "        assert not self.cleared\n",
    "\n",
    "        text_embed = self.clip.encode_text(text)\n",
    "        text_encodings = self.text_encodings\n",
    "        text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n",
    "        del self.text_encodings\n",
    "        return EmbeddedText(l2norm(text_embed.float()), text_encodings.float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_image(self, image):\n",
    "        assert not self.cleared\n",
    "        image = self.validate_and_resize_image(image)\n",
    "        image = self.clip_normalize(image)\n",
    "        image_embed = self.clip.encode_image(image)\n",
    "        return EmbeddedImage(l2norm(image_embed.float()), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2dc311a-addb-45e5-a411-94ae07c15c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_clip = OpenAIClipAdapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fe27b03-a90b-434f-8015-60c2ab6c004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter = iter(train_loader)\n",
    "first_batch = next(train_data_iter)\n",
    "texts = list(first_batch[1])\n",
    "images = first_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "519077b6-57d9-4401-a9dd-9f222a6a227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CLIP embedding generation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenized_texts = clip.tokenize(texts).to(device)\n",
    "clip_text_embeddings = pretrained_clip.embed_text(tokenized_texts)\n",
    "clip_image_embeddings = pretrained_clip.embed_image(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3aeece49-38ab-4ea0-a5de-5dc6f6d6abdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32, 77, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "print(clip_text_embeddings.text_embed.shape)\n",
    "print(clip_text_embeddings.text_encodings.shape)\n",
    "print(clip_image_embeddings.image_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13cdd0-d308-4fd0-ba48-7c679569340a",
   "metadata": {},
   "source": [
    "## Diffusion Prior Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a921f-51f7-4850-8b5b-47c9ac28b11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a325f216-b231-4ca6-92a8-37df14a8a017",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07cfcd34-43f4-4753-ab6b-ead9d341fb3d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Book Virtual Env",
   "language": "python",
   "name": "book-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
