{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b189300-0a5e-4fdf-8662-b87238a29e45",
   "metadata": {},
   "source": [
    "# Chapter 3: Generating Images from Text Using DALL-E 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866743e-e460-47ca-b3b0-cdf171d4d4f6",
   "metadata": {},
   "source": [
    "## Downloading and Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a655c8-27cd-4f5f-ae3b-1c6264c644ab",
   "metadata": {},
   "source": [
    "To download the dataset, visit the link https://cocodataset.org/#download and download the training and validation images for 2014 version along with their annotation files. The annotation files contain the mapping of image IDs to their captions. We will be training our model on the training data while monitoring the validation loss and once the model is trained, we can use it for zero-shot inferences from any data, hence we would not need test images. The MS-COCO dataset released in 2014 contains 164K images split into training (83K), validation (41K) and test (41K) sets. When you would download the dataset, it would download zip files for all the images, which are of different sizes, as well as the annotation files. Unzip the dataset into data/ directory. To load the (image, caption) pairs, we will create PyTorch's data loaders using datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a7e137-db98-4039-b6ba-58e6bdda717e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.3.1\n",
      "  Downloading torch-2.3.1-cp310-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (4.12.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (3.1.5)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.17.0 fsspec-2025.2.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.3 torch-2.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting torchvision==0.17.2\n",
      "  Downloading torchvision-0.17.2-cp310-cp310-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-2.2.3-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch==2.2.2\n",
      "  Downloading torch-2.2.2-cp310-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading pillow-11.1.0-cp310-cp310-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.4.2)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (2025.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from jinja2->torch==2.2.2->torchvision==0.17.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from sympy->torch==2.2.2->torchvision==0.17.2) (1.3.0)\n",
      "Installing collected packages: pillow, numpy, torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "Successfully installed numpy-2.2.3 pillow-11.1.0 torch-2.2.2 torchvision-0.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.8-cp310-cp310-macosx_10_9_universal2.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib>=2.1.0\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-macosx_11_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from pycocotools) (2.2.3)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=8 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (11.1.0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-macosx_11_0_arm64.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-macosx_11_0_arm64.whl (253 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.3/253.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, pycocotools\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.0 pycocotools-2.0.8 pyparsing-3.2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.26.0\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (2.2.3)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.9/408.9 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.29.0 regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.49.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.14-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (0.29.0)\n",
      "Requirement already satisfied: torchvision in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (0.17.2)\n",
      "Requirement already satisfied: torch in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (2.2.2)\n",
      "Requirement already satisfied: safetensors in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (0.5.2)\n",
      "Requirement already satisfied: pyyaml in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2025.2.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: requests in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: jinja2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->timm) (3.1.5)\n",
      "Requirement already satisfied: sympy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->timm) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: numpy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision->timm) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision->timm) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-1.0.14\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -U torch==2.3.1\n",
    "! pip3 install torchvision==0.17.2\n",
    "! pip3 install pycocotools\n",
    "! pip3 install transformers\n",
    "! pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98eecacc-4d35-4ca8-b051-6a0525d99653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatgoel/Documents/Manning Book Multimodal AI/Personal Git Repo/book_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9dc95e5-9c6b-48e6-b8a4-a8275d9983a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transforms\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3667bc5b-add5-4338-81b6-0940bcf9c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "train_dir = '../../Personal Git Repo/data/train2014'\n",
    "val_dir = '../../Personal Git Repo/data/val2014'\n",
    "train_ann_file = '../../Personal Git Repo/data/annotations/captions_train2014.json'\n",
    "val_ann_file = '../../Personal Git Repo/data/annotations/captions_val2014.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e73552e7-0e47-4212-967a-6557992b486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.47s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Images are <class 'PIL.Image.Image'> before transformation\n",
    "train_data = datasets.CocoCaptions(root=train_dir, annFile=train_ann_file, transform=image_transform)\n",
    "val_data = datasets.CocoCaptions(root=val_dir, annFile=val_ann_file, transform=image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13031c9c-0f93-46fc-966b-ffc9df6aec0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  82783\n",
      "Image Size:  torch.Size([3, 224, 224])\n",
      "Image  <class 'torch.Tensor'>\n",
      "['A zebra grazing on lush green grass in a field.', 'Zebra reaching its head down to ground where grass is. ', 'The zebra is eating grass in the sun.', 'A lone zebra grazing in some green grass.', 'a Zebra grazing on grass in a green open field.']\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: ', len(train_data))\n",
    "img, target = train_data[3] # load 4th sample\n",
    "\n",
    "print(\"Image Size: \", img.size())\n",
    "print(\"Image \", type(img))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba2956b-93e6-435c-8c3d-44e5f9a0b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCaptionsFlattened(torch.utils.data.Dataset):\n",
    "    def __init__(self, coco_captions):\n",
    "        self.coco_captions = coco_captions\n",
    "        print(\"Number of images:\", len(self.coco_captions))\n",
    "        caption_counts = [len(captions) for _, captions in coco_captions]\n",
    "        self.cumulative_counts = self._compute_cumulative_counts(caption_counts)\n",
    "        print(\"Number of image x caption pairs:\", self.cumulative_counts[-1])\n",
    "\n",
    "    def _compute_cumulative_counts(self, counts):\n",
    "        cumulative = [0]\n",
    "        for count in counts:\n",
    "            cumulative.append(cumulative[-1] + count)\n",
    "        return cumulative\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_counts[-1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Find the image index corresponding to the flattened caption index\n",
    "        image_idx = bisect.bisect_right(self.cumulative_counts, index) - 1\n",
    "        caption_idx = index - self.cumulative_counts[image_idx]\n",
    "        # print(index, image_idx, caption_idx)\n",
    "        image, captions = self.coco_captions[image_idx]\n",
    "        return image, captions[caption_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02e4005-cc33-4276-8473-796c700283d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 82783\n",
      "Number of image x caption pairs: 414113\n"
     ]
    }
   ],
   "source": [
    "train_data_flattened = CocoCaptionsFlattened(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5602fa0-0046-4392-905a-4b1a9456a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_data_flattened, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963358e-2e1d-421d-b1e9-ad9f44e0c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_flattened = CocoCaptionsFlattened(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb8d1e44-e4ba-49fd-b0a7-744786f2e602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Size:  torch.Size([3, 224, 224])\n",
      "A flower vase is sitting on a porch stand.\n"
     ]
    }
   ],
   "source": [
    "img, caption = train_data_flattened[10] # load 11th sample\n",
    "\n",
    "print(\"Image Size: \", img.size())\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084656e-656e-492a-ab25-16441c0bc31a",
   "metadata": {},
   "source": [
    "## Load CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1132e30-4dd5-4eaf-9e09-3da3b2ba8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClipAdapter(nn.Module):\n",
    "    def __init__(self, clip, **kwargs):\n",
    "        super().__init__()\n",
    "        self.clip = clip\n",
    "        self.overrides = kwargs\n",
    "\n",
    "    def validate_and_resize_image(self, image):\n",
    "        image_size = image.shape[-1]\n",
    "        assert image_size >= self.image_size, f'you are passing in an image of size {image_size} but CLIP requires the image size to be at least {self.image_size}'\n",
    "        return resize_image_to(image, self.image_size)\n",
    "\n",
    "    @property\n",
    "    def dim_latent(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def image_size(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def image_channels(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def max_text_len(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def embed_text(self, text):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def embed_image(self, image):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b886de9-1aab-4974-98e3-d98e0a813099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIClipAdapter(BaseClipAdapter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name = 'ViT-B/32'\n",
    "    ):\n",
    "        import clip\n",
    "        openai_clip, preprocess = clip.load(name)\n",
    "        super().__init__(openai_clip)\n",
    "        self.eos_id = 49407 # for handling 0 being also '!'\n",
    "\n",
    "        text_attention_final = self.find_layer('ln_final')\n",
    "\n",
    "        self.dim_latent_ = text_attention_final.weight.shape[0]\n",
    "        self.handle = text_attention_final.register_forward_hook(self._hook)\n",
    "\n",
    "        self.clip_normalize = preprocess.transforms[-1]\n",
    "        self.cleared = False\n",
    "\n",
    "    def find_layer(self,  layer):\n",
    "        modules = dict([*self.clip.named_modules()])\n",
    "        return modules.get(layer, None)\n",
    "\n",
    "    def clear(self):\n",
    "        if self.cleared:\n",
    "            return\n",
    "\n",
    "        self.handle()\n",
    "\n",
    "    def _hook(self, _, inputs, outputs):\n",
    "        self.text_encodings = outputs\n",
    "\n",
    "    @property\n",
    "    def dim_latent(self):\n",
    "        return self.dim_latent_\n",
    "\n",
    "    @property\n",
    "    def image_size(self):\n",
    "        return self.clip.visual.input_resolution\n",
    "\n",
    "    @property\n",
    "    def image_channels(self):\n",
    "        return 3\n",
    "\n",
    "    @property\n",
    "    def max_text_len(self):\n",
    "        return self.clip.context_length\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_text(self, text):\n",
    "        text = text[..., :self.max_text_len]\n",
    "\n",
    "        is_eos_id = (text == self.eos_id)\n",
    "        text_mask_excluding_eos = is_eos_id.cumsum(dim = -1) == 0\n",
    "        text_mask = F.pad(text_mask_excluding_eos, (1, -1), value = True)\n",
    "        text_mask = text_mask & (text != 0)\n",
    "        assert not self.cleared\n",
    "\n",
    "        text_embed = self.clip.encode_text(text)\n",
    "        text_encodings = self.text_encodings\n",
    "        text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n",
    "        del self.text_encodings\n",
    "        return EmbeddedText(l2norm(text_embed.float()), text_encodings.float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_image(self, image):\n",
    "        assert not self.cleared\n",
    "        image = self.validate_and_resize_image(image)\n",
    "        image = self.clip_normalize(image)\n",
    "        image_embed = self.clip.encode_image(image)\n",
    "        return EmbeddedImage(l2norm(image_embed.float()), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc311a-addb-45e5-a411-94ae07c15c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = OpenAIClipAdapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519077b6-57d9-4401-a9dd-9f222a6a227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CLIP embedding generation\n",
    "clip_text_embeddings = clip.embed_text(caption)\n",
    "clip_image_embeddings = clip.embed_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeece49-38ab-4ea0-a5de-5dc6f6d6abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clip_text_embeddings)\n",
    "print(clip_image_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13cdd0-d308-4fd0-ba48-7c679569340a",
   "metadata": {},
   "source": [
    "## Diffusion Prior Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a921f-51f7-4850-8b5b-47c9ac28b11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a325f216-b231-4ca6-92a8-37df14a8a017",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07cfcd34-43f4-4753-ab6b-ead9d341fb3d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_venv",
   "language": "python",
   "name": "book_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
