{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7ab27d-b948-4701-a583-c8f8314c1eb2",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "## Title: Match Images and Captions Using CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa750d0-356a-4080-ae88-2c1a4314898c",
   "metadata": {},
   "source": [
    "## 2.1 Downloading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddf2b1b-e87c-4376-9513-508b823aa863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.3.1\n",
      "  Using cached torch-2.3.1-cp310-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.3.1) (2025.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from sympy->torch==2.3.1) (1.3.0)\n",
      "Using cached torch-2.3.1-cp310-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.17.2 requires torch==2.2.2, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.3.1\n",
      "Requirement already satisfied: torchvision==0.17.2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision==0.17.2) (1.26.4)\n",
      "Collecting torch==2.2.2 (from torchvision==0.17.2)\n",
      "  Using cached torch-2.2.2-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision==0.17.2) (11.1.0)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (2025.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from jinja2->torch==2.2.2->torchvision==0.17.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from sympy->torch==2.2.2->torchvision==0.17.2) (1.3.0)\n",
      "Using cached torch-2.2.2-cp310-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "Successfully installed torch-2.2.2\n",
      "Requirement already satisfied: pycocotools in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from pycocotools) (3.10.0)\n",
      "Requirement already satisfied: numpy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from pycocotools) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n",
      "Requirement already satisfied: transformers in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: timm in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (1.0.14)\n",
      "Requirement already satisfied: torch in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (0.17.2)\n",
      "Requirement already satisfied: pyyaml in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (0.29.0)\n",
      "Requirement already satisfied: safetensors in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from timm) (0.5.2)\n",
      "Requirement already satisfied: filelock in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->timm) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torch->timm) (3.1.5)\n",
      "Requirement already satisfied: numpy in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from torchvision->timm) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -U torch==2.3.1\n",
    "! pip3 install torchvision==0.17.2\n",
    "! pip3 install pycocotools\n",
    "! pip3 install transformers\n",
    "! pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6197e8-31af-46b6-bc69-9302f88c7ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatgoel/Documents/Manning Book Multimodal AI/Book Git Repo/book-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2dcce0e-6d97-41cc-89f2-6074ceddc702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad4e998-a665-47af-93e1-a30343c93dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transforms\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5534ef34-37e8-4b78-84b4-bf7bcb312d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_dir = '../../Personal Git Repo/data/train2014'\n",
    "val_dir = '../../Personal Git Repo/data/val2014'\n",
    "train_ann_file = '../../Personal Git Repo/data/annotations/captions_train2014.json'\n",
    "val_ann_file = '../../Personal Git Repo/data/annotations/captions_val2014.json'\n",
    "# Images are <class 'PIL.Image.Image'> before transformation\n",
    "train_data = datasets.CocoCaptions(root=train_dir, annFile=train_ann_file, transform=image_transform)\n",
    "val_data = datasets.CocoCaptions(root=val_dir, annFile=val_ann_file, transform=image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "829382c2-2d4a-498d-b722-0139a4855ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  82783\n",
      "Image Size:  torch.Size([3, 224, 224])\n",
      "Image  <class 'torch.Tensor'>\n",
      "['A zebra grazing on lush green grass in a field.', 'Zebra reaching its head down to ground where grass is. ', 'The zebra is eating grass in the sun.', 'A lone zebra grazing in some green grass.', 'a Zebra grazing on grass in a green open field.']\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: ', len(train_data))\n",
    "img, target = train_data[3] # load 4th sample\n",
    "\n",
    "print(\"Image Size: \", img.size())\n",
    "print(\"Image \", type(img))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd44af9c-e0b6-4a4f-9876-64d15f6ef23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCaptionsFlattened(torch.utils.data.Dataset):\n",
    "    def __init__(self, coco_captions):\n",
    "        self.coco_captions = coco_captions\n",
    "        print(\"Number of images:\", len(self.coco_captions))\n",
    "        caption_counts = [len(captions) for _, captions in coco_captions]\n",
    "        self.cumulative_counts = self._compute_cumulative_counts(caption_counts)\n",
    "        print(\"Number of image x caption pairs:\", self.cumulative_counts[-1])\n",
    "\n",
    "    def _compute_cumulative_counts(self, counts):\n",
    "        cumulative = [0]\n",
    "        for count in counts:\n",
    "            cumulative.append(cumulative[-1] + count)\n",
    "        return cumulative\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_counts[-1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Find the image index corresponding to the flattened caption index\n",
    "        image_idx = bisect.bisect_right(self.cumulative_counts, index) - 1\n",
    "        caption_idx = index - self.cumulative_counts[image_idx]\n",
    "        # print(index, image_idx, caption_idx)\n",
    "        image, captions = self.coco_captions[image_idx]\n",
    "        return image, captions[caption_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea6b68e4-9181-480b-9e2e-72453ef91537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 82783\n",
      "Number of image x caption pairs: 414113\n"
     ]
    }
   ],
   "source": [
    "train_data_flattened = CocoCaptionsFlattened(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8977bc0-85ca-497c-909f-b46f856223e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 40504\n",
      "Number of image x caption pairs: 202654\n"
     ]
    }
   ],
   "source": [
    "val_data_flattened = CocoCaptionsFlattened(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d96cc37e-fb63-410c-9a4c-eca197479140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Size:  torch.Size([3, 224, 224])\n",
      "A flower vase is sitting on a porch stand.\n"
     ]
    }
   ],
   "source": [
    "img, caption = train_data_flattened[10] # load 4th sample\n",
    "\n",
    "print(\"Image Size: \", img.size())\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b800a88-c633-47a4-a25a-a307671c4da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414113\n",
      "202654\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_flattened))\n",
    "print(len(val_data_flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4035d3dd-7abb-4e53-9a7e-c53d0c3f7caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ef33e5-fea6-426a-a3b1-405f62e99c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_captions(captions):\n",
    "    tokenized_caption = tokenizer(captions, padding=True, truncation=True, max_length=200)\n",
    "    encoded_captions = {}\n",
    "    for k in tokenized_caption.keys():\n",
    "        encoded_captions[k] = [torch.tensor(value).to(device) for value in tokenized_caption[k]]\n",
    "    return encoded_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4c195e9-350e-4d7e-b76d-3400febbd13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # len batch = 32\n",
    "    images, captions = zip(*batch)\n",
    "    # len images = 32, len captions = 32\n",
    "\n",
    "    # Transfer data to appropriate device\n",
    "    images = [image.to(device) for image in images]\n",
    "    images_batch = torch.stack(images)\n",
    "    \n",
    "    captions_batch = encode_captions(captions)\n",
    "    for k in captions_batch.keys():\n",
    "        captions_batch[k] = torch.stack(captions_batch[k])\n",
    "    \n",
    "    return images_batch, captions_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9326575d-154a-4bec-ad65-59b6492e7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_data_flattened, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_data_flattened, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afa066bc-585a-411d-8f38-e98d2a818e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "32\n",
      "<class 'torch.Tensor'>\n",
      "2\n",
      "32\n",
      "32\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "## For debugging\n",
    "for batch in train_loader:\n",
    "    # Process the batch\n",
    "    # batch is a tuple of size 2, each elem in tuple has size 32 (batch size)\n",
    "    # first elem is a list of image tensors, second elem is a list of dicts (for encoded captions)\n",
    "    print(len(batch))\n",
    "    print(len(batch[0]))\n",
    "    print(type(batch[0]))\n",
    "    print(len(batch[1]))\n",
    "    print(len(batch[1]['input_ids']))\n",
    "    print(len(batch[1]['attention_mask']))\n",
    "    print(type(batch[1]['input_ids']))\n",
    "    print(type(batch[1]['attention_mask']))\n",
    "    break  # To get only one sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa8fa8-ed0b-4396-93d1-cfe56fc3405f",
   "metadata": {},
   "source": [
    "## 2.2 Encoding Images and Text Using CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85642c6b-0e8d-4ba1-9934-d5a8814fbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f90ac473-aaab-4c19-bfd0-c5386ef9fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=\"resnet50\", pretrained=True, trainable=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdd5ad33-8fe6-4c78-a742-c1b177910d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62e4bc5e-03d3-4db1-9076-7fe8f96c5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", trainable=True):\n",
    "        super().__init__()\n",
    "        self.model = DistilBertModel.from_pretrained(model_name)\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "            \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "\n",
    "        # print(\"Normalized last hidden: \", torch.norm(last_hidden_state, p=2, dim=-1))\n",
    "        \n",
    "        # Mean pooling\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "\n",
    "        # print(\"Normalized sum embeddings: \", torch.norm(sum_embeddings, p=2, dim=-1))\n",
    "        # print(\"Embedding variance:\", torch.var(sum_embeddings, dim=-1))\n",
    "        # print(\"Embedding mean:\", torch.mean(sum_embeddings, dim=-1))\n",
    "        \n",
    "        return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a8ac27b-f5b8-4674-993a-5958679ffdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=256,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01c4868c-b150-483c-bed5-103ac585ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=1.0,\n",
    "        image_embedding=2048,\n",
    "        text_embedding=768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    ## TODO: Add snippets of this code between texts\n",
    "    def forward(self, batch):\n",
    "        # batch is a tuple of size 2\n",
    "        # batch[0] is a tensor of 32 image tensors\n",
    "        # batch[1] is a dict, with each value as a tensor of 32 tensors\n",
    "        \n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[0])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[1][\"input_ids\"], attention_mask=batch[1][\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "        image_embeddings = nn.functional.normalize(self.image_projection(image_features), dim=-1)\n",
    "        text_embeddings = nn.functional.normalize(self.text_projection(text_features), dim=-1)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (image_embeddings @ text_embeddings.T) / self.temperature\n",
    "\n",
    "        # Targets (diagonal indices)\n",
    "        batch_size = image_embeddings.size(0)\n",
    "        targets = torch.arange(batch_size, device=logits.device)\n",
    "        \n",
    "        # Compute Loss\n",
    "        loss_image_to_text = self.loss_fn(logits, targets)\n",
    "        loss_text_to_image = self.loss_fn(logits.T, targets)\n",
    "        loss = (loss_image_to_text + loss_text_to_image) / 2.0\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c06edd-ec66-4c41-8d14-661cd4a41699",
   "metadata": {},
   "source": [
    "## 2.3 Training Model to Learn Image-to-Text Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02bdfd33-5151-4588-b391-3ac953db08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03ddf6cf-a1ae-4bdc-af33-0a482f2b16ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 1e-3\n",
    "optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=lr, weight_decay=weight_decay\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff46f18b-28fb-4b18-9069-ae06f57f3aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 2 # check plateau for 2 epochs\n",
    "factor = 0.5 # Reduce learning rate by half\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=patience, factor=factor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d612afd8-7d85-4a38-aee0-dcdc39f01b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bff6e837-9b6c-4b52-a882-b75b5bfa1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_epoch(model, data_loader, optimizer=None, max_batches=None):\n",
    "    if not max_batches:\n",
    "        max_batches = len(data_loader)\n",
    "    \n",
    "    acc_loss = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    tqdm_object = tqdm(data_loader, total=max_batches)\n",
    "\n",
    "    curr_batch = 0\n",
    "    for batch in tqdm_object:\n",
    "        # batch is a tuple of size 2\n",
    "        # batch[0] is a tensor of 32 image tensors\n",
    "        # batch[1] is a dict with each value as tensor of 32 caption tensors\n",
    "        \n",
    "        curr_batch += 1\n",
    "        if curr_batch > max_batches:\n",
    "            break\n",
    "        \n",
    "        loss = model(batch)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        count = len(batch[0]) # number of images in the batch\n",
    "        num_samples += count\n",
    "        acc_loss += loss.item() * count\n",
    "        avg_loss = acc_loss / num_samples\n",
    "\n",
    "        if optimizer:\n",
    "            tqdm_object.set_postfix(train_loss=avg_loss, lr=optimizer.param_groups[0]['lr'])\n",
    "        else:\n",
    "            tqdm_object.set_postfix(valid_loss=avg_loss)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee21604e-e081-450b-b975-3711e0217ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 500/500 [08:25<00:00,  1.01s/it, lr=0.001, train_loss=3.44]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 500/500 [03:35<00:00,  2.32it/s, valid_loss=3.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best Model!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    model.train()\n",
    "    try:\n",
    "        train_loss = model_epoch(model=model, data_loader=train_loader, optimizer=optimizer, max_batches=500)\n",
    "    except Exception as e:\n",
    "        print(\"Exception occured:\", e)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = model_epoch(model=model, data_loader=val_loader, max_batches=500)\n",
    "    \n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"best.pt\")\n",
    "        print(\"Saved Best Model!\")\n",
    "    lr_scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c8204-29ce-46b9-9ee1-181ce5eedf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85754c0e-9b1d-4441-b3d6-fbe24c05eab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c515e1ef-a6bf-4eb3-a48d-1bf2a4fa1735",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f786715-a4dd-4ad0-ba79-34760ad43b10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b0abce1-a34c-45df-ac12-ee086e4a8586",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Book Virtual Env",
   "language": "python",
   "name": "book-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
